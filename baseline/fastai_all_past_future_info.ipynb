{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "import re \n",
    "import matplotlib.pyplot as plt\n",
    "import nvstrings\n",
    "import warnings\n",
    "import cudf as gd\n",
    "import torch\n",
    "import torch.nn\n",
    "import nvcategory\n",
    "\n",
    "from librmm_cffi import librmm\n",
    "from numba import cuda\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from datetime import date\n",
    "from fastai.callbacks import *\n",
    "from fastai import *\n",
    "from fastai.tabular import *\n",
    "from fastai.text import *\n",
    "from fastai.metrics import accuracy\n",
    "from multiprocessing import Process\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GPU_id = 2\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h2> Functions </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#                       #\n",
    "# Metrics and callbacks #\n",
    "#                       #\n",
    "#########################\n",
    "\n",
    "def write_pkl(obj, file_path=None):\n",
    "    if not file_path:\n",
    "        date_time = date.fromtimestamp(time.time()).strftime(\"%m_%d_%Y_%H-%M-%S\")\n",
    "        file_path=f'{date_time}.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "    max_bytes = 2**31 - 1\n",
    "    bytes_out = pickle.dumps(obj, protocol=4)\n",
    "    with open(file_path, 'wb') as f_out:\n",
    "        for idx in range(0, len(bytes_out), max_bytes):\n",
    "            f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "\n",
    "\n",
    "def get_mean_reciprocal_rank(sub):\n",
    "    # sub is a pandas dataframe\n",
    "    # sub should have the following columns:\n",
    "    # 'row_id', 'prob', 'reference', 'item_id'\n",
    "    # sorted by prob in descending order for each group\n",
    "    sub = gd.from_pandas(sub)\n",
    "    \n",
    "    def get_order_in_group(prob,row_id,order):\n",
    "        for i in range(cuda.threadIdx.x, len(prob), cuda.blockDim.x):\n",
    "            order[i] = i\n",
    "\n",
    "    dg = sub.groupby('row_id',method=\"cudf\").apply_grouped(get_order_in_group,incols=['prob','row_id'],\n",
    "                                  outcols={'order': np.int32},\n",
    "                                  tpb=32)\n",
    "\n",
    "    dg = dg.to_pandas()\n",
    "    dg['order'] = 1.0/(1+dg['order'])\n",
    "    dg = dg[dg['reference']==dg['item_id']]\n",
    "    print(dg.isnull().values.any())\n",
    "    return dg['order'].mean()\n",
    "\n",
    "def auroc_score(input, target):\n",
    "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\n",
    "    return roc_auc_score(target, input)\n",
    "\n",
    "# Callback to calculate AUC at the end of each epoch\n",
    "class AUROC(Callback):\n",
    "    _order = -20 #Needs to run before the recorder\n",
    "\n",
    "    def __init__(self, learn, **kwargs): self.learn = learn\n",
    "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\n",
    "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\n",
    "\n",
    "    def on_batch_end(self, last_target, last_output, train, **kwargs):\n",
    "        if not train:\n",
    "            self.output.append(last_output)\n",
    "            self.target.append(last_target)\n",
    "\n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        if len(self.output) > 0:\n",
    "            output = torch.cat(self.output)\n",
    "            target = torch.cat(self.target)\n",
    "            preds = F.softmax(output, dim=1)\n",
    "            metric = auroc_score(preds, target)\n",
    "            return add_metrics(last_metrics, [metric])\n",
    "\n",
    "        \n",
    "def get_idx(x): \n",
    "    return 0 if pd.isnull(x) else id_to_index.get(str(x), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <center> Data Processing </center> </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache/id_to_index.pkl', 'rb') as handle:\n",
    "    id_to_index = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#load tabular data \n",
    "data_pair = pd.read_pickle('cache/data_pair_all.pkl')\n",
    "data_pair = data_pair.drop(columns = [c for c in data_pair.columns if c.startswith('delta') or c.startswith('is')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_context = pd.read_csv('cache/context_info.csv')\n",
    "data_context['past_clickout_item'] = data_context['past_clickout_item'].apply(get_idx)\n",
    "data_context['future_clickout_item'] = data_context['future_clickout_item'].apply(get_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_pair = data_pair.merge(data_context, on='row_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pair['past_clickout_price_diff'] = data_pair['price'] - data_pair['past_clickout_price']\n",
    "data_pair['future_clickout_price_diff'] = data_pair['price'] - data_pair['future_clickout_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train and test dataset \n",
    "train = data_pair[data_pair.clickout_missing==0]\n",
    "test = data_pair[data_pair.clickout_missing>0]\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Input Data </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "cat_names = ['user_id','item_id','platform','city','device','current_filters']\n",
    "cat_names +=['past_clickout_available', \n",
    "             'past_clickout_is_same', \n",
    "             'past_clickout_impression_valid', \n",
    "             'future_clickout_available',\n",
    "             'future_clickout_is_same', \n",
    "             'future_clickout_impression_valid']\n",
    "\n",
    "cont_names = ['price','candidate_order','item_count'] + [i for i in train.columns if i.startswith('count') or 'rank' in i or i.startswith('delta_')]\n",
    "cont_names += [ 'past_clickout_step_diff', \n",
    "                'past_clickout_timestamp_diff',\n",
    "                'past_clickout_price', \n",
    "                'past_clickout_price_mean',  \n",
    "                'past_clickout_price_std',\n",
    "                'past_clickout_price_diff',\n",
    "                'cur_item_count_past',\n",
    "                'future_clickout_step_diff', \n",
    "                'future_clickout_timestamp_diff',\n",
    "                'future_clickout_price',   \n",
    "                'future_clickout_price_mean', \n",
    "                'future_clickout_price_std',\n",
    "                'future_clickout_price_diff',\n",
    "                'cur_item_count_future']\n",
    "\n",
    "procs = [FillMissing,Categorify, Normalize]\n",
    "train['is_va'] = train.row_id%5 == 0\n",
    "\n",
    "test_list = TabularList.from_df(test, path='./', cat_names=cat_names, cont_names=cont_names)\n",
    "\n",
    "data_tab = (TabularList.from_df(train, path='./', cat_names=cat_names, cont_names=cont_names, procs=procs)\n",
    "                           .split_from_df('is_va')\n",
    "                           .label_from_df(cols='target')\n",
    "                           .add_test(test_list)\n",
    "                           .databunch(num_workers=10,bs=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "write_pkl(data_tab)\n",
    "del data_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "learn = tabular_learner(data_tab, layers=[128,64], metrics=None, callback_fns=AUROC,#wd=0.2,\n",
    "        emb_szs = {'user_id': 32,'item_id':32,'platform':4,'city':8,'device':1,\n",
    "                   'current_filters':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr=slice(3e-3), callbacks=[SaveModelCallback(learn,\n",
    "        every='improvement', monitor='AUROC',name='tab_nn')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Performance Evaluation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yp,y_valid = learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cv = train.loc[train['is_va']>0,['row_id','reference','item_id']].copy()\n",
    "cv['prob'] = yp.numpy()[:,1]\n",
    "cv = cv.sort_values(by=['row_id','prob'],ascending=False)\n",
    "print('here')\n",
    "# del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv['row_id'] = cv['row_id'].astype('int32')\n",
    "cv['reference'] = cv['reference'].astype('int32')\n",
    "cv['item_id'] = cv['item_id'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "auc = roc_auc_score(y_valid.numpy().ravel(),yp.numpy()[:,1])\n",
    "mean_reciprocal_rank = get_mean_reciprocal_rank(cv)\n",
    "print('mean_reciprocal_rank %.4f, AUC %.4f'%(mean_reciprocal_rank,auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yps,_ = learn.get_preds(DatasetType.Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test['target'] = yps.numpy()[:,1]\n",
    "test = test['row_id,user_id,session_id,timestamp,step,item_id,target'.split(',')]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = test.sort_values(by=['row_id','target'],ascending=False) # larger probs first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub = test[['row_id','item_id']].copy()\n",
    "sub = sub.groupby('row_id')['item_id'].apply(lambda x: ' '.join([str(i) for i in x]))\n",
    "sub = sub.to_frame()\n",
    "sub.columns = ['new_item_recommendations']\n",
    "sub = sub.reset_index()\n",
    "\n",
    "test = test.drop_duplicates(subset=['row_id'])\n",
    "sub = test.merge(sub,on='row_id',how='left')\n",
    "sub = sub[['session_id','new_item_recommendations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sample_sub = pd.read_csv('/datasets/trivago/data/submission_popular.csv')\n",
    "# sample_sub = pd.read_csv('../input/submission_popular.csv')\n",
    "assert sample_sub.shape[0] == sample_sub.session_id.unique().shape[0]\n",
    "sub = sample_sub.merge(sub,on='session_id',how='left')\n",
    "\n",
    "from datetime import datetime\n",
    "clock = \"{}\".format(datetime.now()).replace(' ','-').replace(':','-').split('.')[0]\n",
    "\n",
    "mask = sub.new_item_recommendations.isnull() == 0\n",
    "sub.loc[mask,'item_recommendations'] = sub.loc[mask,'new_item_recommendations']\n",
    "sub = sub.drop('new_item_recommendations',axis=1)\n",
    "out = 'fastai_%s_mrr_%.4f_auc_%.4f.csv'%(clock,mean_reciprocal_rank,auc)\n",
    "sub.to_csv(out,index=False)\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
